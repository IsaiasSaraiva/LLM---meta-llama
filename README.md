ğŸ¦™ LLM - Meta Llama 3.1 (8B) ğŸš€



ğŸ” Sobre o Projeto
Este repositÃ³rio fornece um ambiente otimizado para executar o Meta-Llama-3.1-8B-Instruct, um modelo de linguagem de alto desempenho. Com suporte a CUDA, bitsandbytes, accelerate e otimizaÃ§Ã£o de memÃ³ria, vocÃª pode rodar inferÃªncia de maneira eficiente em sua GPU. ğŸ’¡ğŸ”¥

ğŸš€ CaracterÃ­sticas

âœ… Modelo: Meta-Llama-3.1-8B-Instruct ğŸ¦™
âœ… Frameworks: PyTorch + Transformers + Accelerate + bitsandbytes
âœ… OtimizaÃ§Ãµes: ReduÃ§Ã£o de memÃ³ria com 4-bit quantization
âœ… Compatibilidade: GPUs NVIDIA com suporte a CUDA
âœ… InferÃªncia rÃ¡pida e eficiente

ğŸ› ï¸ ConfiguraÃ§Ã£o e InstalaÃ§Ã£o

ğŸ”¹ 1. Clone o RepositÃ³rio

 git clone https://github.com/IsaiasSaraiva/LLM---meta-llama
 cd LLM-meta-llama

ğŸ”¹ 2. Crie um Ambiente Virtual (Recomendado)

python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate  # Windows

ğŸ”¹ 3. Instale as DependÃªncias

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers accelerate bitsandbytes

ğŸ”¹ 4. Baixe o Modelo

from transformers import AutoModelForCausalLM, AutoTokenizer

id_model = "meta-llama/Meta-Llama-3.1-8B-Instruct"

model = AutoModelForCausalLM.from_pretrained(id_model, device_map="auto", load_in_4bit=True)
tokenizer = AutoTokenizer.from_pretrained(id_model)

ğŸ”¹ 5. Rode um Exemplo

input_text = "Explique a teoria da relatividade de forma simples."
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")
output = model.generate(**inputs, max_length=200)
print(tokenizer.decode(output[0], skip_special_tokens=True))

